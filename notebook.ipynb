{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx                        # a really useful network analysis library\n",
    "import matplotlib.pyplot as plt\n",
    "# from networkx.algorithms import community   # not used, yet... \n",
    "import datetime                              # access to %%time, for timing individual notebook cells\n",
    "import os\n",
    "import spacy_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section we parse the doc into csv. *Unfortunately, only docx works and doc files are used in python 2.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a Thematic analysis is considered and below is the coding section of the essay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')           # A more detailed model (with higher-dimension word vectors) - 13s to load, normally \n",
    "#nlp = spacy.load('en_core_web_md')           # a smaller model, e.g. for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 20]  # makes the output plots large enough to be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('panel_discussion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "parsed_doc = [] \n",
    "col_to_parse = 'Q1'\n",
    "col2_to_parse = 'Q2'\n",
    "col3_to_parse = 'Q3'\n",
    "col4_to_parse = 'Q4'\n",
    "col5_to_parse = 'AddQ'\n",
    "col6_to_parse = 'LastQ'\n",
    "\n",
    "\n",
    "for doc in nlp.pipe(data[col_to_parse].astype('unicode').values, batch_size=1,\n",
    "                        n_process=1):\n",
    "    if doc.has_annotation(\"DEP\"):\n",
    "        parsed_doc.append(doc)\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "data['parsed_doc'] = parsed_doc\n",
    "data['comment_tokens'] = tokens\n",
    "data['comment_lemma'] = lemma\n",
    "data['pos_pos'] = pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.parsed_doc.to_string(index=False))\n",
    "print(type(data['parsed_doc'][0]))\n",
    "data.Panelist.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next frame is how I got parsed values for each section(i.e Q1, Q2 etc...). SpaCy's NLP pipeline only allows for one column at a time so making csv's and copying the contents was quicker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So its only necessary for multiple values however the data type happens to not be a SpaCy object. For multiple columns, the dataframe returns a string object instead. This is due to NLP pipeline appending the 'str' from the csv and then 'broadcasting' onto vectors readable by Similarity, a SpaCy function which leverages Numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('max_colwidth', None)\n",
    "#blah = data[['parsed_doc', 'comment_tokens', 'comment_lemma', 'pos_pos']].head(50)\n",
    "#with open('file7.csv', mode='w') as file_object:\n",
    "#            print(blah, file=file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print('Number of stopwords: %d' % len(stop_words))\n",
    "print(list(stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a built-in similarity function, thereby correlating how closely related two or more objects such as sentiments are to a 'target object' (in this case the rank in k-space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['parsed_doc'][0].similarity(data['parsed_doc'][1]))\n",
    "#print(data['parsed_doc'][0].similarity(data['parsed_doc'][10]))\n",
    "#print(data['parsed_doc'][1].similarity(data['parsed_doc'][10]))\n",
    "#train_data = nlp(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = data\n",
    "X_data = data[data.Panelist == 'X']\n",
    "Y_data = data[data.Panelist == 'Y']\n",
    "Z_data = data[data.Panelist == 'Z']\n",
    "T_data = data[data.Panelist == 'T']\n",
    "R_data = data[data.Panelist == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 1s for 500 nodes - but of course this won't scale linearly!                              \n",
    "raw_G = nx.Graph() # undirected\n",
    "n = 0\n",
    "\n",
    "for i in tot['parsed_doc']:        # sure, it's inefficient, but it will do\n",
    "    for j in tot['parsed_doc']:\n",
    "        if i != j:\n",
    "            if not (raw_G.has_edge(j, i)):\n",
    "                sim = i.similarity(j)\n",
    "                raw_G.add_edge(i, j, weight = sim)\n",
    "                n = n + 1\n",
    "\n",
    "print(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_kill = []\n",
    "min_wt = 0.96    # this is our cutoff value for a minimum edge-weight \n",
    "\n",
    "for n, nbrs in raw_G.adj.items():\n",
    "    #print(\"\\nProcessing origin-node:\", n, \"... \")\n",
    "    for nbr, eattr in nbrs.items():\n",
    "        # remove edges below a certain weight\n",
    "        data = eattr['weight']\n",
    "        if data < min_wt: \n",
    "            # print('(%.3f)' % (data))  \n",
    "            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n",
    "            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n",
    "            edges_to_kill.append((n, nbr)) \n",
    "            \n",
    "print(\"\\n\", len(edges_to_kill) / 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in edges_to_kill:\n",
    "    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n",
    "        raw_G.remove_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_G = raw_G\n",
    "print(strong_G.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we visualze the graphed nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(strong_G, node_size=20, edge_color='aqua')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "count = strong_G.number_of_nodes()\n",
    "equilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility\n",
    "pos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=200)\n",
    "nx.draw(strong_G, pos=pos, node_size=10, edge_color='tan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 9]  # a better aspect ratio for labelled nodes\n",
    "\n",
    "nx.draw(strong_G, pos, font_size=3, node_size=50, edge_color='tan', with_labels=False)\n",
    "for p in pos:  # raise positions of the labels, relative to the nodes\n",
    "    pos[p][1] -= 0.03\n",
    "nx.draw_networkx_labels(strong_G, pos, font_size=12, font_color='k')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><i>In this analysis, at the edge weight value of 0.989, depicting the most similar values we can see two clusters: 'NaN', which we will ignore and a response from X in question one, thereby emphasizing the significance of sentiments with respect to this qualitative data as a whole.  \"Y mentioned a war technology. How in principle this word could attract many people is because it makes it more real, more concrete. On the other side, Quantum Technology in general is already a sort of science-fiction topic. In that sense, it brings two points.</i> <h3><b>One is the word technology in the case of quantum, may not be so helpful for the general public.</b><b> Because general public is attracted by the aura of mystery behind it.</b> </h3><i>This is however not representative of the whole, just for sentences of the highest similarity. For a highly connected cluster that, we will lower the minimum weight to include more nodes. </i></h4>\n",
    "<h4>It's clear to see a giant cluster at a weight of < 0.985, and at that value the data vectors are similar and legible enough to make sense of what the central node is. \n",
    "From this we can see sentence, \"I think it's extremely important to bring people from outside of the physics department in Quantum Technology\" -From X in question 2. Any weights value lower that the current weight still yeilds this sentence as its strongest connected nodal cluster.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp,\" One is the word technology in the case of quantum, may not be so helpful for the general public.</b><b> Because general public is attracted by the aura of mystery behind it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import explacy\n",
    "explacy.print_parse_info(nlp,\" I think it's extremely important to bring people from outside of the physics department in Quantum Technology.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I think it's extremely important to bring people from outside of the physics department in Quantum Technology.\" This quote contains the highest edge weight and although the fully connected graph contains 32 'similar' points, the above quote contains 12 nodes. It then stands to reason that Panelist X's 2nd response represents the central theme as a whole, and this single sentence emphasizes that point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('Zeki')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a74b34e81fa8895d9d986f09d7875918dd34662d5d0d7b6ce90f79c94e70a994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
